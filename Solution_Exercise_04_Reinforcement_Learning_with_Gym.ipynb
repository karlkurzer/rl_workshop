{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cxgvN1YlSnxK"
   },
   "source": [
    "# Reinforcement Learning\n",
    "\n",
    "![alt text](https://upload.wikimedia.org/wikipedia/commons/thumb/1/1b/Reinforcement_learning_diagram.svg/300px-Reinforcement_learning_diagram.svg.png).\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Reinforcement Learning is a special form of machine learning, where an agent interacts with an environment, conducts observations on the effects of actions and collects rewards.\n",
    "\n",
    "The goal of reinforcement learning is to learn an optimal policy, so that given a state an agent is able to decide what it should do next.\n",
    "\n",
    "In today's workshop we will look into three fundamental algorithms that are capable of solving MDPs, namely [Policy Iteration](https://en.wikipedia.org/wiki/Markov_decision_process#Policy_iteration), [Value Iteration](https://en.wikipedia.org/wiki/Markov_decision_process#Value_iteration), and [Q-Learning](https://en.wikipedia.org/wiki/Q-learning).\n",
    "\n",
    "## Objectives\n",
    "\n",
    "After this workshop you should know:\n",
    "\n",
    "- The relevant pieces for a reinforcement learning system\n",
    "- The basics of *[gym](https://gym.openai.com/envs/#classic_control)* to conduct your own RL experiments\n",
    "- Why Policy Iteration can be slower than Value Iteration (remove this)\n",
    "- The differences of value and policy iteration compared with Q-Learning\n",
    "- How Q-Learning converges towards a stable policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MDP\n",
    "\n",
    "A Markov decision process is a 4-tuple $(S,A,P_{a},R_{a})$\n",
    "\n",
    "![MPD](mdp.png \"MDP\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NvdmBl8GajjF"
   },
   "source": [
    "## Problem\n",
    "\n",
    "Winter is here. You and your friends were tossing around a frisbee at the park when you made a wild throw that left the frisbee out in the middle of the lake. The water is mostly frozen, but there are a few holes where the ice has melted. If you step into one of those holes, you'll fall into the freezing water. At this time, there's an international frisbee shortage, so it's absolutely imperative that you navigate across the lake and retrieve the disc. (However, the ice is slippery, so you won't always move in the direction you intend.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6wYUHIokU_EI"
   },
   "source": [
    "## Setup\n",
    "\n",
    "To begin we'll need to install all the required python package dependencies.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VjQ08kksR2c2"
   },
   "outputs": [],
   "source": [
    "#!pip install --quiet gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8MH3Ij6rAL_z"
   },
   "source": [
    "### Imports and Helper Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JWdytOiH-LFr"
   },
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZgQh5-QCBeDI"
   },
   "outputs": [],
   "source": [
    "from typing import Tuple, List\n",
    "from enum import Enum\n",
    "\n",
    "# Python imports\n",
    "import random\n",
    "import heapq\n",
    "import collections\n",
    "\n",
    "# Reinforcement Learning environments\n",
    "import gymnasium as gym\n",
    "\n",
    "# Scientific computing\n",
    "import numpy as np\n",
    "\n",
    "# Plotting library\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "from matplotlib.colors import LinearSegmentedColormap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALPHA_VALUE = 0.7\n",
    "TEXT_FONT_SIZE = 10\n",
    "\n",
    "\n",
    "class Action(Enum):\n",
    "    \"\"\"\n",
    "    An enumeration for the possible actions in the FrozenLake environment.\n",
    "    \"\"\"\n",
    "\n",
    "    LEFT = 0\n",
    "    DOWN = 1\n",
    "    RIGHT = 2\n",
    "    UP = 3\n",
    "\n",
    "\n",
    "def generate_checkerboard(\n",
    "    img: np.ndarray, v: np.ndarray\n",
    ") -> Tuple[np.ndarray, Tuple[int, int]]:\n",
    "    \"\"\"\n",
    "    Generates a checkerboard pattern by mapping matrix V onto image img.\n",
    "    \"\"\"\n",
    "    size_y, size_x = img.shape[:2]\n",
    "    interpolation_factor_y = size_y // v.shape[0]\n",
    "    interpolation_factor_x = size_x // v.shape[1]\n",
    "\n",
    "    # Broadcasting the smaller matrix V to the size of img\n",
    "    checkerboard = np.repeat(\n",
    "        np.repeat(v, interpolation_factor_y, axis=0), interpolation_factor_x, axis=1\n",
    "    )\n",
    "\n",
    "    return checkerboard, (interpolation_factor_x, interpolation_factor_y)\n",
    "\n",
    "\n",
    "def generate_colormap():\n",
    "    \"\"\"\n",
    "    Generates a default colormap.\n",
    "    \"\"\"\n",
    "    return LinearSegmentedColormap.from_list(\n",
    "        \"custom_cmap\", [(0, \"red\"), (0.5, \"white\"), (1, \"green\")]\n",
    "    )\n",
    "\n",
    "\n",
    "def add_labels(\n",
    "    ax, shape: Tuple[int, int], labels: List, interpolation_factors: Tuple[int, int]\n",
    "):\n",
    "    \"\"\"\n",
    "    Adds labels to the cells of the visualization.\n",
    "    Parameters:\n",
    "        ax (matplotlib.axes.Axes): The axes on which to add labels.\n",
    "        shape (tuple): The shape of the grid.\n",
    "        labels (list): The labels to add.\n",
    "        interpolation_factors (tuple): The interpolation factors for positioning labels.\n",
    "    \"\"\"\n",
    "    for i in range(shape[0]):\n",
    "        for j in range(shape[1]):\n",
    "            ax.text(\n",
    "                interpolation_factors[0] * (j + 0.5),\n",
    "                interpolation_factors[1] * (i + 0.5),\n",
    "                labels[i, j],\n",
    "                ha=\"center\",\n",
    "                va=\"center\",\n",
    "                fontsize=TEXT_FONT_SIZE,\n",
    "                fontweight=\"bold\",\n",
    "                alpha=ALPHA_VALUE,\n",
    "            )\n",
    "\n",
    "\n",
    "def visualize_v(env, v: np.ndarray, ax, title: str) -> None:\n",
    "    \"\"\"Visualizes the value function v of the given environment.\"\"\"\n",
    "    v = v.reshape(env.unwrapped.desc.shape)\n",
    "    V_img, interp_factors = generate_checkerboard(env.render(), v)\n",
    "    V_img = ax.imshow(V_img, cmap=generate_colormap(), alpha=0.5)\n",
    "\n",
    "    labels = np.vectorize(lambda x: f\"{x:.2f}\")(v)\n",
    "\n",
    "    add_labels(ax, env.unwrapped.desc.shape, labels, interp_factors)\n",
    "\n",
    "    visualize_env(env, ax, title)\n",
    "\n",
    "\n",
    "def visualize_p(env, v: np.ndarray, p: np.ndarray, ax, title: str) -> None:\n",
    "    \"\"\"Visualizes the policy p and the of the given environment.\"\"\"\n",
    "    v = v.reshape(env.unwrapped.desc.shape)\n",
    "    V_img, interp_factors = generate_checkerboard(env.render(), v)\n",
    "    V_img = ax.imshow(V_img, cmap=generate_colormap(), alpha=0.5)\n",
    "\n",
    "    s = np.arange(env.unwrapped.observation_space.n)\n",
    "    labels = np.vectorize(lambda x: Action(p[x]).name)(s).reshape(env.unwrapped.desc.shape)\n",
    "\n",
    "    add_labels(ax, env.unwrapped.desc.shape, labels, interp_factors)\n",
    "\n",
    "    visualize_env(env, ax, title)\n",
    "\n",
    "\n",
    "def visualize_env(env, ax, title: str) -> None:\n",
    "    \"\"\"\n",
    "    Visualizes the FrozenLake environment.\n",
    "    \"\"\"\n",
    "    ax.imshow(env.render(), alpha=ALPHA_VALUE)\n",
    "\n",
    "    ax.axis(\"off\")\n",
    "    ax.set_title(title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GzgwlDeZhfxU"
   },
   "source": [
    "\n",
    "#### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mi8myW9Wheef"
   },
   "outputs": [],
   "source": [
    "# Define the default figure size\n",
    "plt.rcParams[\"figure.figsize\"] = [16, 4]\n",
    "\n",
    "def compute_v_from_q(env, q: float) -> float:\n",
    "    \"\"\"Compute the v function given the q function, maximizing over the actions of a given state.\"\"\"\n",
    "    v = np.zeros(env.observation_space.n)\n",
    "    i = 0\n",
    "    for row in env.unwrapped.desc:\n",
    "        j = 0\n",
    "        for _ in row:\n",
    "            s = i * env.unwrapped.desc.shape[0] + j\n",
    "            v[s] = np.max(q[s, :])\n",
    "            j += 1\n",
    "        i += 1\n",
    "    return v\n",
    "\n",
    "\n",
    "def compute_policy_from_q(env, q: float) -> float:\n",
    "    \"\"\"Compute the policy function given the q function, finding the action that yields the maximum of a given state.\"\"\"\n",
    "    policy = np.zeros(env.observation_space.n)\n",
    "    i = 0\n",
    "    for row in env.unwrapped.desc:\n",
    "        j = 0\n",
    "        for _ in row:\n",
    "            s = i * env.unwrapped.desc.shape[0] + j\n",
    "            policy[s] = np.argmax(q[s, :])\n",
    "            j += 1\n",
    "        i += 1\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Policy Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FH2aMLY_jrjQ"
   },
   "outputs": [],
   "source": [
    "def evaluate_episode(env, policy: np.ndarray, discount_factor: float) -> float:\n",
    "    \"\"\"Evaluates a policy by running it until termination and collect its reward\"\"\"\n",
    "    state, _ = env.reset()\n",
    "    total_return = 0\n",
    "    step = 0\n",
    "    while True:\n",
    "        state, reward, done, _, _ = env.step(int(policy[state]))\n",
    "        # Calculate the total\n",
    "        total_return += discount_factor**step * reward\n",
    "        step += 1\n",
    "        if done:\n",
    "            break\n",
    "    return total_return\n",
    "\n",
    "\n",
    "def evaluate_policy(\n",
    "    env, policy: np.ndarray, discount_factor: float, number_episodes: int\n",
    ") -> float:\n",
    "    \"\"\"Evaluates a policy by running it n times\"\"\"\n",
    "    return np.mean(\n",
    "        [evaluate_episode(env, policy, discount_factor) for _ in range(number_episodes)]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Policy and Value Iteraton Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UxwwTshweK8i"
   },
   "outputs": [],
   "source": [
    "# Set parameters\n",
    "MAX_ITERATIONS = 1000\n",
    "NUM_EPISODES = 100\n",
    "DISCOUNT_FACTOR = 0.95"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BzfpVLxA-T4W"
   },
   "source": [
    "### Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Gsl3GswnX1I6"
   },
   "outputs": [],
   "source": [
    "# Deterministic environments\n",
    "env_name = \"FrozenLake-v1\"\n",
    "# env_name = 'FrozenLake8x8-v1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "n8WwK53WADNp"
   },
   "source": [
    "Create the environment with the previously selected name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 382
    },
    "colab_type": "code",
    "id": "ARVwYFcHAB78",
    "outputId": "2f4c131f-cca2-4d82-ebc4-12cfa87f5890"
   },
   "outputs": [],
   "source": [
    "# env = gym.make(env_name)\n",
    "env = gym.make(\n",
    "    env_name,\n",
    "    is_slippery=False,\n",
    "    render_mode=\"rgb_array\",\n",
    ")\n",
    "\n",
    "env.reset()\n",
    "_, ax = plt.subplots()\n",
    "visualize_env(env, ax=ax, title=\"Frozen Lake Environment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Understanding the Environment (Object)\n",
    "\n",
    "**TASK :**\n",
    "Analyze the environment object and figure out its *observation-* and *actionspace* as well as its *reward range*.\n",
    "\n",
    "What is the size of the observation space?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "env.observation_space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the size of the action space?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the range of rewards?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reward_range"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-pzYcAtuiHJ9"
   },
   "source": [
    "### Uncertainty in Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for action in Action:\n",
    "    print('{:15} = {}'.format(action.name, action.value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 568
    },
    "colab_type": "code",
    "id": "F5OmhQ8sVLHK",
    "outputId": "167f15d0-f60c-43af-ebee-14e265d552bd"
   },
   "outputs": [],
   "source": [
    "_, axs = plt.subplots(2, 3, figsize=(12, 6))\n",
    "\n",
    "s, _ = env.reset()\n",
    "print(f\"the initial state is: {s}\")\n",
    "visualize_env(env, ax=axs[0, 0], title=f\"Start | state is: {s}\")\n",
    "\n",
    "axs = axs.reshape(-1)[1:]\n",
    "axs[-1].axis(\"off\")\n",
    "\n",
    "for action, ax in zip(Action, axs):\n",
    "    env.reset()\n",
    "    # skip the first axis\n",
    "    print(f\"executing action {action.value}, should go {action.name}\")\n",
    "    s1, r, d, _, _ = env.step(action.value)\n",
    "    print(f\"new state is: {s1} done: {d}\")\n",
    "    visualize_env(env, ax=ax, title=f\"{action.name} | state is: {s1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_evaluation(env, \n",
    "    policy: np.ndarray, discount_factor: float, mode: str\n",
    ") -> Tuple[np.ndarray, int]:\n",
    "    \"\"\"Iteratively evaluate the value function under the given policy\"\"\"\n",
    "    # Initialize the state value function\n",
    "    v = np.zeros(env.observation_space.n)\n",
    "    iteration = 0\n",
    "    while True:\n",
    "        iteration += 1\n",
    "        prev_v = np.copy(v)\n",
    "        for s in range(env.observation_space.n):\n",
    "            if mode == \"policy_iteration\":\n",
    "                v[s] = evaluate_action(env, s, v, prev_v, policy,discount_factor)\n",
    "            elif mode == \"value_iteration\":\n",
    "                v[s] = evaluate_max_action(env, s, v, prev_v, discount_factor)\n",
    "        if np.sum((np.fabs(prev_v - v))) <= 1e-4:\n",
    "            break\n",
    "    return v, iteration\n",
    "\n",
    "\n",
    "def evaluate_action(\n",
    "        env,\n",
    "    s: int,\n",
    "    v: np.ndarray,\n",
    "    prev_v: np.ndarray,\n",
    "    policy: np.ndarray,\n",
    "    discount_factor: float,\n",
    ") -> float:\n",
    "    # Retrieve the action under the current policy\n",
    "    a = policy[s]\n",
    "    expected_reward = 0\n",
    "    expected_discounted_return = 0\n",
    "    # Calculate the expected reward and the expected discounted return | p = probability\n",
    "    for p, s1, r, _ in env.unwrapped.P[s][a]:\n",
    "        ### TASK: define the expected_reward and the expected_discounted_return\n",
    "        expected_reward += p * r\n",
    "        expected_discounted_return += discount_factor * p * prev_v[s1]\n",
    "    # Calculate the V-Value\n",
    "    return expected_reward + expected_discounted_return\n",
    "\n",
    "\n",
    "def evaluate_max_action(env, \n",
    "    s: int, v: np.ndarray, prev_v: np.ndarray, discount_factor: float\n",
    ") -> float:\n",
    "    # Initialize the action value function\n",
    "    q = np.zeros([env.observation_space.n, env.action_space.n])\n",
    "    # Iterate over each action\n",
    "    for a in range(env.action_space.n):\n",
    "        expected_reward = 0\n",
    "        expected_discounted_return = 0\n",
    "        # Calculate the expected reward and the expected discounted return | p = probability\n",
    "        for p, s1, r, _ in env.unwrapped.P[s][a]:\n",
    "            ### TASK: define the expected_reward and the expected_discounted_return\n",
    "            expected_reward += p * r\n",
    "            expected_discounted_return += discount_factor * p * prev_v[s1]\n",
    "        # Calculate the Q-Value\n",
    "        q[s, a] = expected_reward + expected_discounted_return\n",
    "    ### TASK: define the value function and the policy with respect to q\n",
    "    # Choose the max q value over all actions\n",
    "    return np.max(q[s, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_improvement(env, \n",
    "    v: np.ndarray, policy: np.ndarray, discount_factor: float\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Improve the policy given a value-function\"\"\"\n",
    "    # Initialize the policy\n",
    "    policy = np.zeros(env.observation_space.n)\n",
    "    # Initialize the action value function\n",
    "    q = np.zeros([env.observation_space.n, env.action_space.n])\n",
    "    for s in range(env.observation_space.n):\n",
    "        for a in range(env.action_space.n):\n",
    "            q[s, a] = np.sum(\n",
    "                [p * (r + discount_factor * v[s1]) for p, s1, r, _ in env.unwrapped.P[s][a]]\n",
    "            )\n",
    "        policy[s] = np.argmax(q[s, :])\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CASyoXI9jAZW"
   },
   "source": [
    "## Policy Iteration\n",
    "![Policy Iteration](policy_iteration.png \"Policy Iteration\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm\n",
    "\n",
    "**TASK :**\n",
    "Add the missing steps for the policy iteration algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 137
    },
    "colab_type": "code",
    "id": "B1PWKWKVjQbI",
    "outputId": "5f500ce3-d8d6-49d1-ce4c-aa77b0024a80"
   },
   "outputs": [],
   "source": [
    "def policy_iteration(\n",
    "    env, discount_factor: float, max_iterations: int\n",
    ") -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"Policy-Iteration algorithm\"\"\"\n",
    "    # Initialize the policy\n",
    "    policy = np.zeros(env.observation_space.n) * 2\n",
    "    for i in range(max_iterations):\n",
    "        # TASK: evaluate the current policy\n",
    "        v, iteration = policy_evaluation(\n",
    "            env, policy, discount_factor, \"policy_iteration\"\n",
    "        )\n",
    "        # TASK: define the new policy\n",
    "        new_policy = policy_improvement(env, v, policy, discount_factor)\n",
    "\n",
    "        new_v, _ = policy_evaluation(\n",
    "            env, new_policy, discount_factor, \"policy_iteration\"\n",
    "        )\n",
    "        if np.all(policy == new_policy):\n",
    "            print(f\"Policy-Iteration converged at iteration #{i}\")\n",
    "            break\n",
    "        # Plot the current policy\n",
    "        title_p = f\"Policy Improvement #{i+1}\"\n",
    "        title_v = f\"#Policy Evaluations {iteration}\"\n",
    "        _, ax = plt.subplots(1, 2)\n",
    "        visualize_v(env, v, ax[0], title_v)\n",
    "        visualize_p(env, new_v, new_policy, ax[1], title_p)\n",
    "        # visualize_env(env, v=new_v, p=None, fig=fig, ax=ax[0], title=title_v)\n",
    "        # visualize_env(env, v=new_v, p=new_policy, fig=fig, ax=ax[1], title=title_p)\n",
    "        policy = new_policy\n",
    "    return policy, v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the algorithm and evaluate the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the optimal value function and policy given the model of the environment\n",
    "policy_opt, v_opt = policy_iteration(env, DISCOUNT_FACTOR, 1000)\n",
    "\n",
    "# Evalutate the found value function and policy given the model of the environment\n",
    "policy_return = evaluate_policy(env, policy_opt, DISCOUNT_FACTOR, NUM_EPISODES)\n",
    "print(f\"Average return of the policy: {policy_return:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zhrrLKXk0ElG"
   },
   "source": [
    "## Value Iteration\n",
    "\n",
    "![Value Iteration](value_iteration.png \"Value Iteration\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lZ-utKPdeSw_"
   },
   "source": [
    "### Algorithm\n",
    "**TASK :**\n",
    "Add the missing calculations for the *expected_reward* the *expected_discounted_return*, *v[s]* and *policy[s]*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(env, discount_factor: float, max_iterations: int) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"Value-Iteration algorithm\"\"\"\n",
    "    # Initialize the policy\n",
    "    policy = np.zeros(env.observation_space.n)\n",
    "    for i in range(max_iterations):\n",
    "        # TASK: evaluate the current policy\n",
    "        v, iteration = policy_evaluation(env,\n",
    "            policy, discount_factor, \"value_iteration\"\n",
    "        )\n",
    "        # TASK: define the new policy\n",
    "        new_policy = policy_improvement(env, v, policy, discount_factor)\n",
    "\n",
    "        new_v, _ = policy_evaluation(\n",
    "            env, new_policy, discount_factor, \"policy_iteration\"\n",
    "        )\n",
    "        if np.all(policy == new_policy):\n",
    "            print(f\"Policy-Iteration converged at iteration #{i}\")\n",
    "            break\n",
    "        # Plot the current policy\n",
    "        title_p = f\"Policy Improvement #{i + 1}\"\n",
    "        title_v = f\"#Policy Evaluations {iteration}\"\n",
    "        _, ax = plt.subplots(1, 2)\n",
    "        visualize_v(env, v, ax[0], title_v)\n",
    "        visualize_p(env, new_v, new_policy, ax[1], title_p)\n",
    "        policy = new_policy\n",
    "    return policy, v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the algorithm and evaluate the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the optimal value function and policy given the model of the environment\n",
    "policy_opt, v_opt = value_iteration(env, DISCOUNT_FACTOR, 1000)\n",
    "\n",
    "# Evalutate the found value function and policy given the model of the environment\n",
    "policy_return = evaluate_policy(env, policy_opt, DISCOUNT_FACTOR, NUM_EPISODES)\n",
    "print(f\"Average return of the policy: {policy_return:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lU4gmOQcAjR_"
   },
   "source": [
    "## Q-Learning\n",
    "\n",
    "![Q-Learning](q_learning.png \"Q-Learning\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Temporal Difference Error\n",
    "### $\\delta_t = \\underbrace{r_{t}}_{\\text{reward}} + \\underbrace{\\gamma}_{\\text{discount factor}} \\cdot \\underbrace{\\max_{a}Q(s_{t+1}, a)}_{\\text{estimate of optimal future value}} - \\underbrace{Q(s_{t}, a_{t})}_{\\text{estimate of optimal current value}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Temporal Difference Update\n",
    "### $Q^{new}(s_{t},a_{t}) \\leftarrow \\underbrace{Q(s_{t},a_{t})}_{\\text{old value}} + \\underbrace{\\alpha}_{\\text{learning rate}} \\cdot \\underbrace{\\delta_t}_\\text{temporal difference error}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transiton Tuple\n",
    "For ease of use we define a transition tuple that allows us to combine all the relevant information from one state to another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# p = priority (only needed for (prioritized) experience replay)\n",
    "# s = state\n",
    "# a = action\n",
    "# s1 = successor state\n",
    "# r = reward\n",
    "# td_e = temporal difference error\n",
    "Transition = collections.namedtuple(\"Transition\", (\"p\", \"s\", \"a\", \"s1\", \"r\", \"td_e\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replay Memory and Prioritized Experience Replay (optional)\n",
    "\n",
    "Experience Replay and prioritization of specific experiences are common techniques to make the training more data efficient.\n",
    "\n",
    "* [Paper - Experience Replay, 1992](https://link.springer.com/content/pdf/10.1007%2FBF00992699.pdf)\n",
    "* [Paper - Prioritized Experience Replay, 2015](https://arxiv.org/abs/1511.05952)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory:\n",
    "    def __init__(self, config):\n",
    "        # transitions memory\n",
    "        self.transitions = []\n",
    "        # size of the memory\n",
    "        self.memory_size = config.memory_size\n",
    "        # size of the batches\n",
    "        self.batch_size = config.batch_size\n",
    "        # flag for prioritized experience replay\n",
    "        self.prioritized = config.prioritized\n",
    "\n",
    "    def push(self, transition: Transition):\n",
    "        # if the memory is not yet full add the new transition\n",
    "        if len(self.transitions) < self.memory_size:\n",
    "            heapq.heappush(self.transitions, transition)\n",
    "        # if the memory is full remove the smallest transition and add the new transition\n",
    "        else:\n",
    "            del self.transitions[-1]\n",
    "            heapq.heappush(self.transitions, transition)\n",
    "\n",
    "    def replay(self):\n",
    "        if self.prioritized:\n",
    "            return heapq.nsmallest(self.batch_size, self.transitions)\n",
    "        else:\n",
    "            return random.sample(sorted(self.transitions), self.batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.transitions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Q Agent\n",
    "So far we have only defined simple function calls with\n",
    "```python\n",
    "def function_name(arg1, arg2):\n",
    "    # compute something with arg1 and arg2 and return something\n",
    "    if arg2 > 0:\n",
    "        something = other_function(arg1) - arg2\n",
    "    else:\n",
    "        something = arg1\n",
    "    return something\n",
    "```\n",
    "However for more complex tasks it is advisable to write object oriented code using classes. Classes provide a means of bundling data and functionality together. Creating a new class creates a new type of object, allowing new instances of that type to be made. Each class instance can have attributes attached to it for maintaining its state. Class instances can also have methods (defined by its class) for modifying its state.\n",
    "\n",
    "\n",
    "\n",
    "Hence we create a class **QAgent** that incorporates all the methods needed for Q-Learning.\n",
    "\n",
    "```python\n",
    "class QAgent:\n",
    "    \n",
    "    def __init__(self): # constructor method that gets called when the object is being created\n",
    "        \n",
    "    def td_error(self): # Temporal Difference Error\n",
    "        \n",
    "    def td_update(self): # Temporal Difference Update\n",
    "    \n",
    "    def train(self, env): # Train the agent     \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASK :**\n",
    "Add the missing formulars for the TD-error and the TD-update."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QAgent:\n",
    "    def __init__(self, config):\n",
    "        # Maximum length of training\n",
    "        self.training_length = config.training_length\n",
    "        # Maximum length of an episode\n",
    "        self.episode_length = config.episode_length\n",
    "        # TD error update step size\n",
    "        self.learning_rate = config.learning_rate\n",
    "        # TD error update step size\n",
    "        self.discount_factor = config.discount_factor\n",
    "        # Enabling experience replay\n",
    "        self.replay_memory_enabled = True if config.config_replay_memory else False\n",
    "        # Initialize the replay memory of the agent\n",
    "        if self.replay_memory_enabled:\n",
    "            self.replay_memory = ReplayMemory(config.config_replay_memory)\n",
    "\n",
    "    def td_error(self, q: float, s: int, a: int, s1: int, r: float) -> float:\n",
    "        \"\"\"Calculates the temporal difference error given the current model and transition\"\"\"\n",
    "        # TASK: return the TD-Error\n",
    "        td_e = r + self.discount_factor * np.max(q[s1, :]) - q[s, a]\n",
    "        return td_e\n",
    "\n",
    "    def td_update(self, q: float, t) -> float:\n",
    "        \"\"\"Calculates the adjusted action value (q) given the td error from a single transition\"\"\"\n",
    "        # TASK: return the update for the q value\n",
    "        q = q + self.learning_rate * t.td_e\n",
    "        return q\n",
    "\n",
    "    def td_replay(self, q: float, q_target: float) -> float:\n",
    "        # Use the replay memory to run additional updates\n",
    "        if len(self.replay_memory) >= self.replay_memory.batch_size:\n",
    "            for t in self.replay_memory.replay(self.replay_memory.batch_size):\n",
    "                # Recalculate the temporal difference error for this transition\n",
    "                td_e = self.td_error(q_target, t.s, t.a, t.s1, t.r)\n",
    "                # Create an updated transition tuple\n",
    "                updated_t = Transition(-td_e, t.s, t.a, t.s1, t.r, td_e)\n",
    "                # Save the transition in replay memory\n",
    "                self.replay_memory.push(updated_t)\n",
    "                # Update model / q table\n",
    "                q[t.s, t.a] = self.td_update(q_target[t.s, t.a], updated_t)\n",
    "        return q\n",
    "\n",
    "    def epsilon_greedy_noise(self, env, s: int, episode: int) -> Tuple[int, float]:\n",
    "        epsilon = np.random.randn(1, env.action_space.n) * (1.0 / (episode + 1))\n",
    "        a = np.argmax(self.q_target[s, :] + epsilon)\n",
    "        return a, epsilon\n",
    "\n",
    "    def epsilon_greedy_linear(self, env, s: int, episode: int) -> Tuple[int, float]:\n",
    "        epsilon = 1 - (episode + 1) / self.training_length\n",
    "        if epsilon > np.random.rand():\n",
    "            a = np.random.randint(env.action_space.n)\n",
    "        else:\n",
    "            a = np.argmax(self.q_target[s, :])\n",
    "        return a, epsilon\n",
    "\n",
    "    def train(self, env):\n",
    "        # Initialize the model / q table with zeros/random\n",
    "        self.q = np.zeros([env.observation_space.n, env.action_space.n])\n",
    "        # Create a target model / q table\n",
    "        self.q_target = self.q\n",
    "\n",
    "        ### METRICS\n",
    "        # create lists to contain various metrics that should be tracked during the training process\n",
    "        self.metrics = {\n",
    "            \"return\": np.zeros(self.training_length),\n",
    "            \"q_avg\": np.zeros(self.training_length),\n",
    "            \"epsilon\": np.zeros(self.training_length),\n",
    "            \"td_error\": np.zeros(self.training_length),\n",
    "        }\n",
    "\n",
    "        for episode in range(self.training_length):\n",
    "            # Reset the environment and retrieve the initial state\n",
    "            s, _ = env.reset()\n",
    "            # Set the 'done' flag to false\n",
    "            d = False\n",
    "            # Set the step of the episode to 0\n",
    "            step = 0\n",
    "            # Start the Q-Learning algorithm\n",
    "            while step < self.episode_length:\n",
    "                # Derive action from current policy (epsilon_greedy noise)\n",
    "                # a, epsilon = self.epsilon_greedy_linear(env, s , episode)\n",
    "                a, epsilon = self.epsilon_greedy_noise(env, s, episode)\n",
    "\n",
    "                # Execute the action and generate a succesor state as well as receive an immediate reward\n",
    "                s1, r, d, _, _ = env.step(a)\n",
    "\n",
    "                # Calculate the temporal difference error\n",
    "                td_e = self.td_error(self.q_target, s, a, s1, r)\n",
    "\n",
    "                # Create a transition tuple\n",
    "                transition = Transition(-(td_e + 0.001), s, a, s1, r, td_e)\n",
    "\n",
    "                # Save the transition in replay memory\n",
    "                if self.replay_memory_enabled:\n",
    "                    self.replay_memory.push(transition)\n",
    "\n",
    "                # Update model / q table\n",
    "                self.q[s, a] = self.td_update(self.q_target[s, a], transition)\n",
    "\n",
    "                # Assign the current state the value of the successor state\n",
    "                s = s1\n",
    "\n",
    "                # Increment the step\n",
    "                step += 1\n",
    "\n",
    "                ### METRICS\n",
    "                # Accumulate the episode return\n",
    "                self.metrics[\"return\"][episode] += self.discount_factor**step * r\n",
    "                # Track the temporal difference error\n",
    "                self.metrics[\"td_error\"][episode] += td_e\n",
    "                # Track the max epsilon values\n",
    "                self.metrics[\"epsilon\"][episode] += np.max(epsilon)\n",
    "                # Track the average q values\n",
    "                self.metrics[\"q_avg\"][episode] = np.average(self.q)\n",
    "\n",
    "                # If we reached a terminal state abort the while loop reset the environment and start over\n",
    "                if d == True or step == 100:\n",
    "                    # At the end of the episode update the target model with the current model\n",
    "\n",
    "                    # If experience replay is enabled replay the experience collected so far\n",
    "                    if self.replay_memory_enabled:\n",
    "                        self.q_target = self.td_replay(self.q, self.q_target)\n",
    "                    else:\n",
    "                        self.q_target = self.q\n",
    "\n",
    "                    ### METRICS\n",
    "                    self.metrics[\"epsilon\"][episode] /= step\n",
    "                    self.metrics[\"q_avg\"][episode] /= step\n",
    "                    self.metrics[\"td_error\"][episode] /= step\n",
    "                    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration Tuple\n",
    "For ease of use we define a configuration tuple that allows us to combine all the relevant configuration from into one object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ConfigQAgent = collections.namedtuple(\n",
    "    \"ConfigQAgent\",\n",
    "    (\n",
    "        \"learning_rate\",\n",
    "        \"training_length\",\n",
    "        \"episode_length\",\n",
    "        \"discount_factor\",\n",
    "        \"config_replay_memory\",\n",
    "    ),\n",
    ")\n",
    "\n",
    "ConfigReplayMemory = collections.namedtuple(\n",
    "    \"ConfigReplayMemory\", (\"memory_size\", \"batch_size\", \"prioritized\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure and Train the Agent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agent 1\n",
    "config_replay_memory = ConfigReplayMemory(500, 50, False)\n",
    "config_q_agent = ConfigQAgent(0.1, 400, 50, DISCOUNT_FACTOR, None)\n",
    "\n",
    "q_agent = QAgent(config_q_agent)\n",
    "q_agent.train(env)\n",
    "policy = compute_policy_from_q(env, q_agent.q_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Average return: {evaluate_policy(env, policy, q_agent.discount_factor, 1000):.2f}\")\n",
    "print(\n",
    "    f\"Score over time: {sum(q_agent.metrics['return']) / q_agent.training_length:.2f}\"\n",
    ")\n",
    "\n",
    "fig, axi = plt.subplots(1, 2)\n",
    "v = compute_v_from_q(env, q_agent.q_target)\n",
    "visualize_p(env, v, policy, axi[0], title=\"Policy for the Frozen Lake\")\n",
    "visualize_v(env, v, axi[1], title=\"State Value Function for the Frozen Lake\")\n",
    "\n",
    "fig, ax = plt.subplots(1, 4)\n",
    "# Plot the return over time\n",
    "ax[0].plot(range(q_agent.training_length), q_agent.metrics[\"return\"], \".\")\n",
    "ax[0].set(xlabel=\"episode\", ylabel=\"reward\", title=\"Return\")\n",
    "ax[0].grid()\n",
    "\n",
    "# Plot the Q value over time\n",
    "ax[1].plot(range(q_agent.training_length), q_agent.metrics[\"q_avg\"], \".\")\n",
    "ax[1].set(xlabel=\"episode\", ylabel=\"Q Value\", title=\"Average Q Value\")\n",
    "ax[1].grid()\n",
    "\n",
    "# Plot the epsilon over time\n",
    "ax[2].plot(range(q_agent.training_length), q_agent.metrics[\"epsilon\"], \".\")\n",
    "ax[2].set(xlabel=\"episode\", ylabel=\"epsilon\", title=\"Epsilon\")\n",
    "ax[2].grid()\n",
    "\n",
    "# Plot the td error over time\n",
    "ax[3].plot(range(q_agent.training_length), q_agent.metrics[\"td_error\"], \".\")\n",
    "ax[3].set(xlabel=\"episode\", ylabel=\"TD Error\", title=\"TD Error\")\n",
    "ax[3].grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate different Hyperparameters (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_training_episodes(number_evaluation_points: int, number_evaluations: int):\n",
    "    ### Evaluate different episode lengths\n",
    "    training_lengths = np.linspace(1, 501, number_evaluation_points, dtype=int)\n",
    "    returns = np.zeros(number_evaluation_points)\n",
    "    for i in range(number_evaluation_points):\n",
    "        config_q_agent = ConfigQAgent(\n",
    "            0.1, training_lengths[i], 100, DISCOUNT_FACTOR, None\n",
    "        )\n",
    "        for j in range(number_evaluations):\n",
    "            q_agent = QAgent(config_q_agent)\n",
    "            q_agent.train(env)\n",
    "            returns[i] += np.max(q_agent.metrics[\"return\"])\n",
    "        returns[i] /= number_evaluations\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.set(xlabel=\"#Episodes\", ylabel=\"Max. Return\", title=\"#Episodes vs. Return\")\n",
    "    ax.plot(training_lengths, returns, \"-o\")\n",
    "\n",
    "\n",
    "evaluate_training_episodes(10, 10)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "6wYUHIokU_EI",
    "8MH3Ij6rAL_z",
    "JWdytOiH-LFr",
    "GzgwlDeZhfxU",
    "rTC-P1vd-5-y",
    "-pzYcAtuiHJ9",
    "zhrrLKXk0ElG",
    "CASyoXI9jAZW",
    "lU4gmOQcAjR_",
    "4CdfVP4DilJf",
    "wK6bzLs_iqeG",
    "5KUNPRHdAstO",
    "tny1fTdaIkR6"
   ],
   "name": "Exercise 04 - Reinforcement Learning with Gym and Pytorch.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "rl-kernel",
   "language": "python",
   "name": "rl-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
