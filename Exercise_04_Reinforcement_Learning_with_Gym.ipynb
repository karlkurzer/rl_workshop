{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cxgvN1YlSnxK"
   },
   "source": [
    "# Reinforcement Learning\n",
    "\n",
    "![alt text](https://upload.wikimedia.org/wikipedia/commons/thumb/1/1b/Reinforcement_learning_diagram.svg/300px-Reinforcement_learning_diagram.svg.png).\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Reinforcement Learning is a special form of machine learning, where an agent interacts with an environment, conducts observations on the effects of actions and collects rewards.\n",
    "\n",
    "The goal of reinforcement learning is to learn an optimal policy, so that given a state an agent is able to decide what it should do next.\n",
    "\n",
    "In this exercise we will look into three fundamental algorithms that are capable of solving MDPs, namely [Policy Iteration](https://en.wikipedia.org/wiki/Markov_decision_process#Policy_iteration), [Value Iteration](https://en.wikipedia.org/wiki/Markov_decision_process#Value_iteration), and [ Q-Learning](https://en.wikipedia.org/wiki/Q-learning).\n",
    "\n",
    "## Objectives\n",
    "\n",
    "By the time you complete this lab, you should know:\n",
    "\n",
    "- The relevant pieces for a reinforcement learning system\n",
    "- The basics of *[gym](https://gym.openai.com/envs/#classic_control)* to conduct your own RL experiments\n",
    "- Why Policy Iteration can be slower than Value Iteration\n",
    "- The differences of value and policy iteration compared with Q-Learning\n",
    "- How Q-Learning converges towards a stable policy\n",
    "    - Some optional extensions to Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MDP\n",
    "\n",
    "A Markov decision process is a 4-tuple $(S,A,P_{a},R_{a})$\n",
    "\n",
    "![MPD](mdp.png \"MDP\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NvdmBl8GajjF"
   },
   "source": [
    "## Problem\n",
    "\n",
    "Winter is here. You and your friends were tossing around a frisbee at the park when you made a wild throw that left the frisbee out in the middle of the lake. The water is mostly frozen, but there are a few holes where the ice has melted. If you step into one of those holes, you'll fall into the freezing water. At this time, there's an international frisbee shortage, so it's absolutely imperative that you navigate across the lake and retrieve the disc. (However, the ice is slippery, so you won't always move in the direction you intend.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6wYUHIokU_EI"
   },
   "source": [
    "## Setup\n",
    "\n",
    "To begin we'll need to install all the required python package dependencies.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VjQ08kksR2c2"
   },
   "outputs": [],
   "source": [
    "#!pip install --quiet gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8MH3Ij6rAL_z"
   },
   "source": [
    "### Imports and Helper Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JWdytOiH-LFr"
   },
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZgQh5-QCBeDI"
   },
   "outputs": [],
   "source": [
    "# Python imports\n",
    "import random\n",
    "import heapq\n",
    "import collections\n",
    "\n",
    "# Reinforcement Learning environments\n",
    "import gym\n",
    "# Scientific computing\n",
    "import numpy as np\n",
    "# Plotting library\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GzgwlDeZhfxU"
   },
   "source": [
    "\n",
    "#### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mi8myW9Wheef"
   },
   "outputs": [],
   "source": [
    "# Define the default figure size\n",
    "plt.rcParams['figure.figsize'] = [16, 4]\n",
    "\n",
    "def create_numerical_map(env):\n",
    "    \"\"\"Convert the string map of the environment to a numerical version\"\"\"\n",
    "    numerical_map = np.zeros(env.env.desc.shape)\n",
    "    i = 0\n",
    "    for row in env.env.desc:\n",
    "        j = 0\n",
    "        for col in row:\n",
    "            if col.decode('UTF-8') == 'S':\n",
    "                numerical_map[i, j] = 0\n",
    "            elif col.decode('UTF-8') == 'G':\n",
    "                numerical_map[i, j] = 1\n",
    "            elif col.decode('UTF-8') == 'F':\n",
    "                numerical_map[i, j] = 2\n",
    "            elif col.decode('UTF-8') == 'H':\n",
    "                numerical_map[i, j] = 3\n",
    "            j += 1\n",
    "        i += 1\n",
    "    return numerical_map\n",
    "\n",
    "\n",
    "def visualize_env(env):\n",
    "    \"\"\"Plot the environment\"\"\"\n",
    "    fig, ax = plt.subplots()\n",
    "    # Hide grid lines\n",
    "    ax.grid(False)\n",
    "    # Hide axes ticks\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.set_title('The frozen Lake')\n",
    "    i = ax.imshow(create_numerical_map(env), cmap=cm.jet)\n",
    "    plt.show()\n",
    "    print('the start is blue, holes are red, ice is yellow and the goal is teal')\n",
    "\n",
    "def visualize_policy(env, policy, ax=None, title=None):\n",
    "    \"\"\"Plot the policy in the environment\"\"\"\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "    font_size = 10 if env.observation_space.n > 16 else 20\n",
    "    i = 0\n",
    "    for row in env.env.desc:\n",
    "        j = 0\n",
    "        for col in row:\n",
    "            s = i * env.env.desc.shape[0]+j\n",
    "            if policy[s] == 0:\n",
    "                ax.annotate(\"L\", xy=(j, i), xytext=(j, i), ha=\"center\",\n",
    "                            va=\"center\", size=font_size, color=\"white\")\n",
    "            elif policy[s] == 1:\n",
    "                ax.annotate(\"D\", xy=(j, i), xytext=(j, i), ha=\"center\",\n",
    "                            va=\"center\", size=font_size, color=\"white\")\n",
    "            elif policy[s] == 2:\n",
    "                ax.annotate(\"R\", xy=(j, i), xytext=(j, i), ha=\"center\",\n",
    "                            va=\"center\", size=font_size, color=\"white\")\n",
    "            elif policy[s] == 3:\n",
    "                ax.annotate(\"U\", xy=(j, i), xytext=(j, i), ha=\"center\",\n",
    "                            va=\"center\", size=font_size, color=\"white\")\n",
    "            j += 1\n",
    "        i += 1\n",
    "\n",
    "    # Hide grid lines\n",
    "    ax.grid(False)\n",
    "    # Hide axes ticks\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    if title is None:\n",
    "        ax.set_title('Policy for the Frozen Lake')\n",
    "    else:\n",
    "        ax.set_title(title)\n",
    "    ax.imshow(create_numerical_map(env), cmap=cm.jet)\n",
    "    return\n",
    "\n",
    "\n",
    "def visualize_v(env, v, ax=None, title=None):\n",
    "    \"\"\"Plot value function values in the environment\"\"\"\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "    font_size = 10 if env.observation_space.n > 16 else 20\n",
    "    i = 0\n",
    "    for row in env.env.desc:\n",
    "        j = 0\n",
    "        for col in row:\n",
    "            s = i * env.env.desc.shape[0]+j\n",
    "            ax.annotate(\"{:.2f}\".format(v[s]), xy=(j, i), xytext=(j, i), ha=\"center\",\n",
    "                        va=\"center\", size=font_size, color=\"white\")\n",
    "            j += 1\n",
    "        i += 1\n",
    "\n",
    "    # Hide grid lines\n",
    "    ax.grid(False)\n",
    "    # Hide axes ticks\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    if title is None:\n",
    "        ax.set_title('State Value Function for the Frozen Lake')\n",
    "    else:\n",
    "        ax.set_title(title)\n",
    "    ax.imshow(create_numerical_map(env), cmap=cm.jet)\n",
    "    return\n",
    "\n",
    "\n",
    "def compute_v_from_q(env, q):\n",
    "    \"\"\"Compute the v function given the q function, maximizing over the actions of a given state.\"\"\"\n",
    "    v = np.zeros(env.observation_space.n)\n",
    "    i = 0\n",
    "    for row in env.env.desc:\n",
    "        j = 0\n",
    "        for col in row:\n",
    "            s = i * env.env.desc.shape[0]+j\n",
    "            v[s] = np.max(q[s, :])\n",
    "            j += 1\n",
    "        i += 1\n",
    "    return v\n",
    "\n",
    "def compute_policy_from_q(env, q):\n",
    "    \"\"\"Compute the policy function given the q function, finding the action that yields the maximum of a given state.\"\"\"\n",
    "    policy = np.zeros(env.observation_space.n)\n",
    "    i = 0\n",
    "    for row in env.env.desc:\n",
    "        j = 0\n",
    "        for col in row:\n",
    "            s = i * env.env.desc.shape[0]+j\n",
    "            policy[s] = np.argmax(q[s, :])\n",
    "            j += 1\n",
    "        i += 1\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rTC-P1vd-5-y"
   },
   "source": [
    "#### Deterministic Environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sknx1oOiaL7J"
   },
   "outputs": [],
   "source": [
    "# register variants of the frozen lake without execution uncertainty i.e. deterministic environments\n",
    "from gym.envs.registration import register\n",
    "\n",
    "register(\n",
    "    id='FrozenLakeNotSlippery-v0',\n",
    "    entry_point='gym.envs.toy_text:FrozenLakeEnv',\n",
    "    kwargs={'map_name': '4x4', 'is_slippery': False},\n",
    "    max_episode_steps=100,\n",
    "    reward_threshold=0.78,  # optimum = .8196\n",
    ")\n",
    "\n",
    "register(\n",
    "    id='FrozenLakeNotSlippery8x8-v0',\n",
    "    entry_point='gym.envs.toy_text:FrozenLakeEnv',\n",
    "    kwargs={'map_name': '8x8', 'is_slippery': False},\n",
    "    max_episode_steps=200,\n",
    "    reward_threshold=0.99,  # optimum = 1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Policy Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FH2aMLY_jrjQ"
   },
   "outputs": [],
   "source": [
    "def evaluate_episode(env, policy, discount_factor):\n",
    "    \"\"\"Evaluates a policy by running it until termination and collect its reward\"\"\"\n",
    "    state = env.reset()\n",
    "    total_return = 0\n",
    "    step = 0\n",
    "    while True:\n",
    "        state, reward, done, _ = env.step(int(policy[state]))\n",
    "        # Calculate the total\n",
    "        total_return += (discount_factor ** step * reward)\n",
    "        step += 1\n",
    "        if done:\n",
    "            break\n",
    "    return total_return\n",
    "\n",
    "\n",
    "def evaluate_policy(env, policy, discount_factor=0.95, number_episodes=1000):\n",
    "    \"\"\" Evaluates a policy by running it n times\"\"\"\n",
    "    return np.mean([evaluate_episode(env, policy, discount_factor) for _ in range(number_episodes)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Policy and Value Iteraton Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UxwwTshweK8i"
   },
   "outputs": [],
   "source": [
    "# Set parameters\n",
    "max_iterations = 1000\n",
    "num_episodes = 100\n",
    "discount_factor = 0.95"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BzfpVLxA-T4W"
   },
   "source": [
    "### Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Gsl3GswnX1I6"
   },
   "outputs": [],
   "source": [
    "# Deterministic environments\n",
    "env_name = 'FrozenLakeNotSlippery-v0'\n",
    "#env_name = 'FrozenLakeNotSlippery8x8-v0'\n",
    "\n",
    "# Stochastic environments\n",
    "#env_name = 'FrozenLake-v0'\n",
    "#env_name = 'FrozenLake8x8-v0'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "n8WwK53WADNp"
   },
   "source": [
    "Create the environment with the previously selected name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 382
    },
    "colab_type": "code",
    "id": "ARVwYFcHAB78",
    "outputId": "2f4c131f-cca2-4d82-ebc4-12cfa87f5890",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "env = gym.make(env_name)\n",
    "print('Generated the frozen lake with config: ' + env_name)\n",
    "visualize_env(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Understanding the Environment (Object)\n",
    "\n",
    "**TASK :**\n",
    "Analyze the environment object and figure out its *observation-* and *actionspace* as well as its *reward range*.\n",
    "\n",
    "What is the size of the observation space?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "env.observation_space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the size of the action space?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the range of rewards?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reward_range"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-pzYcAtuiHJ9"
   },
   "source": [
    "### Uncertainty in Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 568
    },
    "colab_type": "code",
    "id": "F5OmhQ8sVLHK",
    "outputId": "167f15d0-f60c-43af-ebee-14e265d552bd",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "s = env.reset()\n",
    "print(\"the initial state is: {}\".format(s))\n",
    "env.render()\n",
    "\n",
    "# The agent should go down\n",
    "print(\"executing action 1, should go down\")\n",
    "s1, r, d, _ = env.step(1)\n",
    "print(\"new state is: {} done: {}\".format(s1, d))\n",
    "env.render()\n",
    "\n",
    "# The agent should go up\n",
    "print(\"executing action 3, should go up\")\n",
    "s1, r, d, _ = env.step(3)\n",
    "print(\"new state is: {} done: {}\".format(s1, d))\n",
    "env.render()\n",
    "\n",
    "# The agent should go right\n",
    "print(\"executing action 2, should go right\")\n",
    "s1, r, d, _ = env.step(2)\n",
    "print(\"new state is: {} done: {}\".format(s1, d))\n",
    "env.render()\n",
    "\n",
    "# The agent should go left\n",
    "print(\"executing action 0, should go left\")\n",
    "s1, r, d, _ = env.step(0)\n",
    "print(\"new state is: {} done: {}\".format(s1, d))\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_evaluation(policy, env, discount_factor, mode):\n",
    "    \"\"\" Iteratively evaluate the value function under the given policy\"\"\"\n",
    "    # Initialize the state value function\n",
    "    v = np.zeros(env.observation_space.n)\n",
    "    iteration = 0\n",
    "    while True:\n",
    "        iteration += 1\n",
    "        prev_v = np.copy(v)\n",
    "        for s in range(env.env.nS):\n",
    "            if mode == \"policy_iteration\":\n",
    "                v[s] = evaluate_action(s, v, prev_v, policy, env, discount_factor)\n",
    "            elif mode == \"value_iteration\":\n",
    "                v[s] = evaluate_max_action(s, v, prev_v, env, discount_factor)\n",
    "        if (np.sum((np.fabs(prev_v - v))) <= 1e-4):\n",
    "            break\n",
    "    return v, iteration\n",
    "\n",
    "def evaluate_action(s, v, prev_v, policy, env, discount_factor):\n",
    "    # Retrieve the action under the current policy\n",
    "    a = policy[s]\n",
    "    expected_reward = 0\n",
    "    expected_discounted_return = 0\n",
    "    # Calculate the expected reward and the expected discounted return | p = probability\n",
    "    for p, s1, r, _ in env.env.P[s][a]:\n",
    "        ### TASK: define the expected_reward and the expected_discounted_return\n",
    "        expected_reward += \n",
    "        expected_discounted_return += \n",
    "    # Calculate the V-Value\n",
    "    return expected_reward + expected_discounted_return\n",
    "\n",
    "\n",
    "def evaluate_max_action(s, v, prev_v, env, discount_factor):\n",
    "    # Initialize the action value function\n",
    "    q = np.zeros([env.observation_space.n, env.action_space.n])\n",
    "    # Iterate over each action\n",
    "    for a in range(env.action_space.n):\n",
    "        expected_reward = 0\n",
    "        expected_discounted_return = 0\n",
    "        # Calculate the expected reward and the expected discounted return | p = probability\n",
    "        for p, s1, r, _ in env.env.P[s][a]:\n",
    "            ### TASK: define the expected_reward and the expected_discounted_return\n",
    "            expected_reward += \n",
    "            expected_discounted_return += \n",
    "        # Calculate the Q-Value\n",
    "        q[s, a] = expected_reward + expected_discounted_return\n",
    "    ### TASK: define the value function with respect to q\n",
    "    # Choose the max Q-Value over all actions\n",
    "    return q[s, a]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_improvement(v, policy, env, discount_factor):\n",
    "    \"\"\" Improve the policy given a value-function \"\"\"\n",
    "    # Initialize the policy\n",
    "    policy = np.zeros(env.observation_space.n)\n",
    "    # Initialize the action value function\n",
    "    q = np.zeros([env.observation_space.n, env.action_space.n])\n",
    "    for s in range(env.observation_space.n):\n",
    "        for a in range(env.action_space.n):\n",
    "            q[s,a] = np.sum([p * (r + discount_factor * v[s1]) for p, s1, r, _ in  env.env.P[s][a]])\n",
    "        policy[s] = np.argmax(q[s,:])\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CASyoXI9jAZW"
   },
   "source": [
    "## Policy Iteration\n",
    "![Policy Iteration](policy_iteration.png \"Policy Iteration\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm\n",
    "\n",
    "**TASK :**\n",
    "Add the missing steps for the policy iteration algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 137
    },
    "colab_type": "code",
    "id": "B1PWKWKVjQbI",
    "outputId": "5f500ce3-d8d6-49d1-ce4c-aa77b0024a80",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def policy_iteration(env, discount_factor, max_iterations):\n",
    "    \"\"\" Policy-Iteration algorithm \"\"\"\n",
    "    # Initialize the policy\n",
    "    policy = np.zeros(env.observation_space.n)*2\n",
    "    for i in range(max_iterations):\n",
    "        # TASK: evaluate the current policy\n",
    "        v, iteration = \n",
    "        # TASK: define the new policy\n",
    "        new_policy = \n",
    "        if (np.all(policy == new_policy)):\n",
    "            print ('Policy-Iteration converged at iteration #{:d}'.format((i)))\n",
    "            break\n",
    "        # Plot the current policy\n",
    "        title_p = 'Policy Improvement #{:d}'.format((i+1))\n",
    "        title_v = '#Policy Evaluations {:d}'.format(iteration)\n",
    "        fig, ax = plt.subplots(1,2)\n",
    "        visualize_v(env, v, ax[0], title_v)\n",
    "        visualize_policy(env, new_policy, ax[1], title_p)\n",
    "        policy = new_policy\n",
    "    return policy, v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the algorithm and evaluate the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Determine the optimal value function and policy given the model of the environment\n",
    "policy_opt, v_opt = policy_iteration(env, discount_factor, 1000)\n",
    "\n",
    "# Evalutate the found value function and policy given the model of the environment\n",
    "policy_return = evaluate_policy(env, policy_opt, discount_factor, num_episodes)\n",
    "print('Average return of the policy: {:.2f}'.format(policy_return))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zhrrLKXk0ElG"
   },
   "source": [
    "## Value Iteration\n",
    "\n",
    "![Value Iteration](value_iteration.png \"Value Iteration\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lZ-utKPdeSw_"
   },
   "source": [
    "### Algorithm\n",
    "**TASK :**\n",
    "Add the missing calculations for the *expected_reward* the *expected_discounted_return*, *v[s]* and *policy[s]*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(env, discount_factor, max_iterations):\n",
    "    \"\"\" Value-Iteration algorithm \"\"\"\n",
    "    # Initialize the policy\n",
    "    policy = np.zeros(env.observation_space.n)\n",
    "    for i in range(max_iterations):\n",
    "        # TASK: evaluate the current policy\n",
    "        v, iteration = \n",
    "        # TASK: define the new policy\n",
    "        new_policy = \n",
    "        if (np.all(policy == new_policy)):\n",
    "            print ('Policy-Iteration converged at iteration #{:d}'.format((i)))\n",
    "            break\n",
    "        # Plot the current policy\n",
    "        title_p = 'Policy Improvement #{:d}'.format((i+1))\n",
    "        title_v = '#Policy Evaluations {:d}'.format(iteration)\n",
    "        fig, ax = plt.subplots(1,2)\n",
    "        visualize_v(env, v, ax[0], title_v)\n",
    "        visualize_policy(env, new_policy, ax[1], title_p)\n",
    "        policy = new_policy\n",
    "    return policy, v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the algorithm and evaluate the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the optimal value function and policy given the model of the environment\n",
    "policy_opt, v_opt = value_iteration(env, discount_factor, 1000)\n",
    "\n",
    "# Evalutate the found value function and policy given the model of the environment\n",
    "policy_return = evaluate_policy(env, policy_opt, discount_factor, num_episodes)\n",
    "print('Average return of the policy: {:.2f}'.format(policy_return))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lU4gmOQcAjR_"
   },
   "source": [
    "## Q-Learning\n",
    "\n",
    "![Q-Learning](q_learning.png \"Q-Learning\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Temporal Difference Error\n",
    "### $\\delta_t = \\underbrace{r_{t}}_{\\text{reward}} + \\underbrace{\\gamma}_{\\text{discount factor}} \\cdot \\underbrace{\\max_{a}Q(s_{t+1}, a)}_{\\text{estimate of optimal future value}} - \\underbrace{Q(s_{t}, a_{t})}_{\\text{estimate of optimal current value}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Temporal Difference Update\n",
    "### $Q^{new}(s_{t},a_{t}) \\leftarrow \\underbrace{Q(s_{t},a_{t})}_{\\text{old value}} + \\underbrace{\\alpha}_{\\text{learning rate}} \\cdot \\underbrace{\\delta_t}_\\text{temporal difference error}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transiton Tuple\n",
    "For ease of use we define a transition tuple that allows us to combine all the relevant information from one state to another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# p = priority (only needed for (prioritized) experience replay)\n",
    "# s = state\n",
    "# a = action\n",
    "# s1 = successor state\n",
    "# r = reward\n",
    "# td_e = temporal difference error\n",
    "Transition = collections.namedtuple('Transition', ('p', 's', 'a', 's1', 'r', 'td_e'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replay Memory and Prioritized Experience Replay (optional)\n",
    "\n",
    "Experience Replay and prioritization of specific experiences are common techniques to make the training more data efficient.\n",
    "\n",
    "* [Paper - Experience Replay, 1992](https://link.springer.com/content/pdf/10.1007%2FBF00992699.pdf)\n",
    "* [Paper - Prioritized Experience Replay, 2015](https://arxiv.org/abs/1511.05952)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory():\n",
    "    def __init__(self, config):\n",
    "        # transitions memory\n",
    "        self.transitions = []\n",
    "        # size of the memory\n",
    "        self.memory_size = config.memory_size\n",
    "        # size of the batches\n",
    "        self.batch_size = config.batch_size\n",
    "        # flag for prioritized experience replay\n",
    "        self.prioritized = config.prioritized\n",
    "        \n",
    "    def push(self, transition):\n",
    "        # if the memory is not yet full add the new transition\n",
    "        if len(self.transitions) < self.memory_size:\n",
    "            heapq.heappush(self.transitions, transition)\n",
    "        # if the memory is full remove the smallest transition and add the new transition \n",
    "        else:\n",
    "            del self.transitions[-1]\n",
    "            heapq.heappush(self.transitions, transition)\n",
    "    \n",
    "    def replay(self, batch_size):\n",
    "        if self.prioritized:\n",
    "            return heapq.nsmallest(self.batch_size,self.transitions)\n",
    "        else: \n",
    "            return random.sample(sorted(self.transitions),self.batch_size)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.transitions)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Q Agent\n",
    "So far we have only defined simple function calls with\n",
    "```python\n",
    "def function_name(arg1, arg2):\n",
    "    # compute something with arg1 and arg2 and return something\n",
    "    if arg2 > 0:\n",
    "        something = other_function(arg1) - arg2\n",
    "    else:\n",
    "        something = arg1\n",
    "    return something\n",
    "```\n",
    "However for more complex tasks it is advisable to write object oriented code using classes. Classes provide a means of bundling data and functionality together. Creating a new class creates a new type of object, allowing new instances of that type to be made. Each class instance can have attributes attached to it for maintaining its state. Class instances can also have methods (defined by its class) for modifying its state.\n",
    "\n",
    "\n",
    "\n",
    "Hence we create a class **QAgent** that incorporates all the methods needed for Q-Learning.\n",
    "\n",
    "```python\n",
    "class QAgent:\n",
    "    \n",
    "    def __init__(self): # constructor method that gets called when the object is being created\n",
    "        \n",
    "    def td_error(self): # Temporal Difference Error\n",
    "        \n",
    "    def td_update(self): # Temporal Difference Update\n",
    "    \n",
    "    def train(self, env): # Train the agent     \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASK :**\n",
    "Add the missing formulars for the TD-error and the TD-update."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QAgent:\n",
    "    def __init__(self, config):\n",
    "        # Maximum length of training\n",
    "        self.training_length = config.training_length\n",
    "        # Maximum length of an episode \n",
    "        self.episode_length = config.episode_length\n",
    "        # TD error update step size\n",
    "        self.learning_rate = config.learning_rate\n",
    "        # TD error update step size\n",
    "        self.discount_factor = config.discount_factor\n",
    "        # Enabling experience replay\n",
    "        self.replay_memory_enabled = True if config.config_replay_memory else False\n",
    "        # Initialize the replay memory of the agent\n",
    "        if self.replay_memory_enabled:\n",
    "            self.replay_memory = ReplayMemory(config.config_replay_memory)\n",
    "        \n",
    "    def td_error(self, q, s, a, s1, r):\n",
    "        # TASK: return the TD-Error\n",
    "        # Calculates the temporal difference error given the current model and transition\n",
    "        td_e = \n",
    "        return td_e\n",
    "    \n",
    "    def td_update(self, q, t):\n",
    "        # TASK: return the update for the q value\n",
    "        # Calculates the adjusted action value (q) given the td error from a single transition\n",
    "        q = \n",
    "        return q\n",
    "\n",
    "    def td_replay(self, q, q_target):\n",
    "        # Use the replay memory to run additional updates\n",
    "        if len(self.replay_memory) >= self.replay_memory.batch_size:\n",
    "            for t in self.replay_memory.replay(self.replay_memory.batch_size):\n",
    "                # Recalculate the temporal difference error for this transition\n",
    "                td_e = self.td_error(q_target, t.s, t.a, t.s1, t.r)\n",
    "                # Create an updated transition tuple\n",
    "                updated_t = Transition(-td_e, t.s, t.a, t.s1, t.r, td_e)\n",
    "                # Save the transition in replay memory\n",
    "                self.replay_memory.push(updated_t)\n",
    "                # Update model / q table\n",
    "                q[t.s, t.a] = self.td_update(q_target[t.s, t.a], updated_t)\n",
    "        return q\n",
    "    \n",
    "    def epsilon_greedy_noise(self, episode):\n",
    "        epsilon = np.random.randn(1, env.action_space.n)*(1./(episode+1))\n",
    "        a = np.argmax(self.q_target[s, :] + epsilon)\n",
    "        return a, epsilon\n",
    "    \n",
    "    def epsilon_greedy_linear(self, env, episode):\n",
    "        epsilon = (1-(episode+1)/self.training_length)\n",
    "        if epsilon > np.random.rand():\n",
    "            a = np.random.randint(env.action_space.n)\n",
    "        else:\n",
    "            a = np.argmax(self.q_target[s, :])\n",
    "        return a, epsilon\n",
    "    \n",
    "    def train(self, env):\n",
    "        # Initialize the model / q table with zeros/random\n",
    "        self.q = np.zeros([env.observation_space.n, env.action_space.n])\n",
    "        # Create a target model / q table\n",
    "        self.q_target = self.q\n",
    "        \n",
    "        ### METRICS\n",
    "        # create lists to contain various metrics that should be tracked during the training process\n",
    "        self.metrics = {\n",
    "            'return': np.zeros(self.training_length),\n",
    "            'q_avg': np.zeros(self.training_length),\n",
    "            'epsilon': np.zeros(self.training_length),\n",
    "            'td_error': np.zeros(self.training_length)\n",
    "        }\n",
    "        \n",
    "        for episode in range(self.training_length):\n",
    "            # Reset the environment and retrieve the initial state\n",
    "            s = env.reset()\n",
    "            # Set the 'done' flag to false\n",
    "            d = False\n",
    "            # Set the step of the episode to 0\n",
    "            step = 0\n",
    "            # Start the Q-Learning algorithm\n",
    "            while step < self.episode_length:\n",
    "                # Derive action from current policy (epsilon_greedy noise)\n",
    "                epsilon = np.random.randn(1, env.action_space.n)*(1./(episode+1))\n",
    "                a = np.argmax(self.q_target[s, :] + epsilon)\n",
    "\n",
    "                # Execute the action and generate a succesor state as well as receive an immediate reward\n",
    "                s1, r, d, _ = env.step(a)\n",
    "\n",
    "                # Calculate the temporal difference error\n",
    "                td_e = self.td_error(self.q_target, s, a, s1, r)\n",
    "\n",
    "                # Create a transition tuple\n",
    "                transition = Transition(-(td_e+0.001), s, a, s1, r, td_e)       \n",
    "\n",
    "                # Save the transition in replay memory\n",
    "                if self.replay_memory_enabled:\n",
    "                    self.replay_memory.push(transition)\n",
    "\n",
    "                # Update model / q table\n",
    "                self.q[s, a] = self.td_update(self.q_target[s, a], transition)\n",
    "\n",
    "                # Assign the current state the value of the successor state\n",
    "                s = s1\n",
    "\n",
    "                # Increment the step\n",
    "                step += 1\n",
    "\n",
    "                ### METRICS\n",
    "                # Accumulate the episode return\n",
    "                self.metrics['return'][episode] += self.discount_factor**step*r\n",
    "                # Track the temporal difference error\n",
    "                self.metrics['td_error'][episode] += td_e\n",
    "                # Track the max epsilon values\n",
    "                self.metrics['epsilon'][episode] += np.max(epsilon)\n",
    "                # Track the average q values\n",
    "                self.metrics['q_avg'][episode] = np.average(self.q)\n",
    "\n",
    "\n",
    "                # If we reached a terminal state abort the while loop reset the environment and start over\n",
    "                if d == True or step == 100:\n",
    "\n",
    "                    # At the end of the episode update the target model with the current model\n",
    "\n",
    "                    # If experience replay is enabled replay the experience collected so far\n",
    "                    if self.replay_memory_enabled:\n",
    "                        self.q_target = self.td_replay(self.q, self.q_target)\n",
    "                    else:\n",
    "                        self.q_target = self.q\n",
    "\n",
    "                    ### METRICS\n",
    "                    self.metrics['epsilon'][episode] /= step\n",
    "                    self.metrics['q_avg'][episode] /= step\n",
    "                    self.metrics['td_error'][episode] /= step\n",
    "                    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration Tuple\n",
    "For ease of use we define a configuration tuple that allows us to combine all the relevant configuration from into one object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ConfigQAgent = collections.namedtuple('ConfigQAgent', ('learning_rate',\n",
    "                                                       'training_length',\n",
    "                                                       'episode_length',                                                       'discount_factor',\n",
    "                                                       'config_replay_memory')\n",
    "                                     )\n",
    "\n",
    "ConfigReplayMemory = collections.namedtuple('ConfigReplayMemory', ('memory_size', \n",
    "                                                                   'batch_size', \n",
    "                                                                   'prioritized')\n",
    "                                           )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure and Train the Agent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Agent 1\n",
    "config_replay_memory = ConfigReplayMemory(500, 50, False)\n",
    "config_q_agent = ConfigQAgent(0.1, 400, 50, discount_factor, None)\n",
    "\n",
    "q_agent = QAgent(config_q_agent)\n",
    "q_agent.train(env)\n",
    "policy = compute_policy_from_q(env, q_agent.q_target)     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Average return: {:.2f}'.format(evaluate_policy(env, policy, q_agent.discount_factor, 1000)))\n",
    "print(\"Score over time: \" + str(sum(q_agent.metrics['return'])/q_agent.training_length))\n",
    "\n",
    "fig, axi = plt.subplots(1, 2)\n",
    "visualize_policy(env, policy, axi[0])\n",
    "visualize_v(env, compute_v_from_q(env, q_agent.q_target),axi[1])\n",
    "\n",
    "fig, ax = plt.subplots(1, 4)\n",
    "# Plot the return over time\n",
    "ax[0].plot(range(q_agent.training_length), q_agent.metrics['return'], \".\")\n",
    "ax[0].set(xlabel='episode', ylabel='reward', title='Return')\n",
    "ax[0].grid()\n",
    "\n",
    "# Plot the Q value over time\n",
    "ax[1].plot(range(q_agent.training_length), q_agent.metrics['q_avg'], \".\")\n",
    "ax[1].set(xlabel='episode', ylabel='Q Value', title='Average Q Value')\n",
    "ax[1].grid()\n",
    "\n",
    "# Plot the epsilon over time\n",
    "ax[2].plot(range(q_agent.training_length), q_agent.metrics['epsilon'], \".\")\n",
    "ax[2].set(xlabel='episode', ylabel='epsilon', title='Epsilon')\n",
    "ax[2].grid()\n",
    "\n",
    "# Plot the td error over time\n",
    "ax[3].plot(range(q_agent.training_length), q_agent.metrics['td_error'], \".\")\n",
    "ax[3].set(xlabel='episode', ylabel='TD Error', title='TD Error')\n",
    "ax[3].grid()   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate different Hyperparameters (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_training_episodes(number_evaluation_points, number_evaluations):\n",
    "    ### Evaluate different episode lengths\n",
    "    training_lengths = np.linspace(1, 501, number_evaluation_points, dtype = int)\n",
    "    returns = np.zeros(number_evaluation_points)\n",
    "    for i in range(number_evaluation_points):\n",
    "        config_q_agent = ConfigQAgent(0.1, training_lengths[i], 100, discount_factor, None)\n",
    "        for j in range(number_evaluations):\n",
    "            q_agent = QAgent(config_q_agent)\n",
    "            q_agent.train(env)\n",
    "            returns[i] += np.max(q_agent.metrics['return'])\n",
    "        returns[i] /= number_evaluations\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.set(xlabel='#Episodes', ylabel='Max. Return', title='#Episodes vs. Return')\n",
    "    ax.plot(training_lengths, returns, '-o');\n",
    "    \n",
    "evaluate_training_episodes(10, 10)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "6wYUHIokU_EI",
    "8MH3Ij6rAL_z",
    "JWdytOiH-LFr",
    "GzgwlDeZhfxU",
    "rTC-P1vd-5-y",
    "-pzYcAtuiHJ9",
    "zhrrLKXk0ElG",
    "CASyoXI9jAZW",
    "lU4gmOQcAjR_",
    "4CdfVP4DilJf",
    "wK6bzLs_iqeG",
    "5KUNPRHdAstO",
    "tny1fTdaIkR6"
   ],
   "name": "Exercise 04 - Reinforcement Learning with Gym and Pytorch.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "ml_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
